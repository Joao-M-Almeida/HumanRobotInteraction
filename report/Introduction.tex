
\section{Introduction}
The main goal of this project is to make a mobile robot display human-aware behaviours, based on the predicted behaviours of the human the robot is interacting with. 

This is a topic that has been subject to a lot of interest lately, recent works like~\cite{hof} have been done concerning the prediction of humans intention.

In~\cite{wang}, an intention driven approach is proposed to make inference with specific tasks where the agent needs to act quickly, like playing table tennis, for example. 

Other works~\cite{towards,shared} concern more about creating a natural interaction, such that the robot has to understand the goal of the humans action in order to select the best way to deal with that. For example, analyzing the humans field of vision and actions. 

The approach of this work will be more similar to these last ones, focusing in creating a natural human-robot interaction, with some level of prediction. 

Following~\cite{Gesture recognition,body} it is intended with this work to create a model able to understand each captured gesture.

Bearing this approach in mind it is our goal to prove some assumptions/approximations taken as a basis for this work:  

\begin{itemize}
\item People tend to interact with other people with well defined and clear sequences of movements and gestures;
\item The sequences of gestures tend to be similar for a given situation even across different people;
\item A Machine Learning Classifier will be able to capture information that would be too hard to explicitly program. $\rightarrow$ This system should have improved results over explicitly programmed solutions;
\end{itemize}

With these items in mind we developed a setup to test our approach. The robot detects gestures and movements of the user and based on the sequence tries to predict what next action should be taken.